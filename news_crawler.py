#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import re
import time
import json
import base64
import logging
import pathlib
import urllib.parse
import requests
import email.utils
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from tqdm import tqdm
import colorama
from collections import defaultdict

# Google API ê´€ë ¨ ëª¨ë“ˆ
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build

# í™˜ê²½ ë³€ìˆ˜ ë° ë¡œê¹… ì„¤ì •
load_dotenv()
LOG_DIR = pathlib.Path("logs")
LOG_DIR.mkdir(parents=True, exist_ok=True)
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
LOG_FILE = LOG_DIR / f"news_crawler_{TIMESTAMP}.log"

logging.basicConfig(
    level=logging.INFO,
    handlers=[
        logging.FileHandler(LOG_FILE, mode="a", encoding="utf-8", errors="replace")
    ],
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("news_crawler")

# í„°ë¯¸ë„ ìƒ‰ìƒ ë° tqdm ì§„í–‰ë°” ì„¤ì •
colorama.init(autoreset=True)
COLORS = {
    "RED": "\033[91m",
    "GREEN": "\033[92m",
    "YELLOW": "\033[93m",
    "BLUE": "\033[94m",
    "MAGENTA": "\033[95m",
    "CYAN": "\033[96m",
    "WHITE": "\033[97m",
    "RESET": "\033[0m",
}


def colored_bar_format(color: str) -> str:
    return f"{color}{{desc}}: {{percentage:3.0f}}%|{{bar:30}}| {{n_fmt}}/{{total_fmt}} [{{elapsed}}<{{remaining}}, {{rate_fmt}}{{postfix}}]{COLORS['RESET']}"


def get_tqdm_settings(
    desc: str, color: str, position: int, leave: bool = False
) -> dict:
    return {
        "desc": desc,
        "ncols": 0,
        "bar_format": colored_bar_format(color),
        "dynamic_ncols": True,
        "mininterval": 0.3,
        "position": position,
        "leave": leave,
    }


# =============================================================================
# NewsFetcher í´ë˜ìŠ¤: ë‰´ìŠ¤ ê²€ìƒ‰, ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ, ìš”ì•½, ìœ ì‚¬ë„ ë° ì¤‘ë³µ ì œê±° ê¸°ëŠ¥
# =============================================================================
class NewsFetcher:
    def __init__(self):
        self.news_cache: dict = {}
        self.similarity_cache: dict = {}
        self.api_usage_stats: dict = {
            "total_tokens": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_cost_usd": 0.0,
            "total_cost_krw": 0.0,
            "api_calls": 0,
            "models": {},
        }
        self.model_prices = {
            "gpt-4o-mini": {"input": 0.15, "output": 0.60},
            "gpt-4o": {"input": 5.0, "output": 15.0},
            "gpt-4": {"input": 10.0, "output": 30.0},
            "gpt-3.5-turbo": {"input": 0.5, "output": 1.5},
        }

    @staticmethod
    def preprocess_title(title: str) -> str:
        title = re.sub(r"\s*[-â€“]\s*[\w\s]+$", "", title)
        title = re.sub(r"[^\w\s]", " ", title)
        title = re.sub(r"\s+", " ", title)
        return title.strip().lower()

    @staticmethod
    def simple_jaccard_similarity(text1: str, text2: str) -> float:
        prep1 = NewsFetcher.preprocess_title(text1)
        prep2 = NewsFetcher.preprocess_title(text2)
        tokens1, tokens2 = set(prep1.split()), set(prep2.split())
        if not tokens1 or not tokens2:
            return 0.0
        intersection = len(tokens1 & tokens2)
        union = len(tokens1 | tokens2)
        weight = (
            1.2 if min(len(tokens1), len(tokens2)) <= 3 and intersection > 0 else 1.0
        )
        similarity = (intersection / union) * weight
        return min(similarity, 1.0)

    def track_api_usage(
        self,
        model: str,
        prompt_tokens: int,
        completion_tokens: int,
        total_tokens: int,
        usd_cost: float,
        krw_cost: float,
    ) -> None:
        stats = self.api_usage_stats
        stats["total_tokens"] += total_tokens
        stats["prompt_tokens"] += prompt_tokens
        stats["completion_tokens"] += completion_tokens
        stats["total_cost_usd"] += usd_cost
        stats["total_cost_krw"] += krw_cost
        stats["api_calls"] += 1

        if model not in stats["models"]:
            stats["models"][model] = {
                "total_tokens": 0,
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_cost_usd": 0.0,
                "total_cost_krw": 0.0,
                "api_calls": 0,
            }
        model_stats = stats["models"][model]
        model_stats["total_tokens"] += total_tokens
        model_stats["prompt_tokens"] += prompt_tokens
        model_stats["completion_tokens"] += completion_tokens
        model_stats["total_cost_usd"] += usd_cost
        model_stats["total_cost_krw"] += krw_cost
        model_stats["api_calls"] += 1

    def _update_api_usage_from_result(self, result: dict, model: str) -> None:
        if "usage" in result:
            prompt_tokens = result["usage"].get("prompt_tokens", 0)
            completion_tokens = result["usage"].get("completion_tokens", 0)
            total_tokens = result["usage"].get("total_tokens", 0)
            price_info = self.model_prices.get(model, {"input": 0.15, "output": 0.60})
            input_cost = (prompt_tokens / 1_000_000) * price_info["input"]
            output_cost = (completion_tokens / 1_000_000) * price_info["output"]
            total_cost = input_cost + output_cost
            exchange_rate = float(os.getenv("USD_TO_KRW", 1350))
            krw_cost = total_cost * exchange_rate
            self.track_api_usage(
                model,
                prompt_tokens,
                completion_tokens,
                total_tokens,
                total_cost,
                krw_cost,
            )

    def get_google_news(
        self,
        query: str = "",
        country: str = "kr",
        language: str = "ko",
        max_items: int = 30,
    ) -> list:
        cache_key = f"{query}_{country}_{language}"
        if cache_key in self.news_cache:
            logger.info(f"ìºì‹œì—ì„œ í† í”½ '{query}' ë‰´ìŠ¤ ë¡œë“œ")
            return self.news_cache[cache_key]

        base_url = "https://news.google.com/rss"
        url = f"{base_url}/search?q={urllib.parse.quote(query)}" if query else base_url
        url += f"&hl={language}-{country}&gl={country}&ceid={country}:{language}"
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                logger.error(f"HTTP ìƒíƒœ ì½”ë“œ {response.status_code}")
                return []
            root = ET.fromstring(response.content)
            news_items = []
            two_days_ago = datetime.now(timezone.utc) - timedelta(days=2)
            for item in root.findall(".//item"):
                title_elem = item.find("title")
                link_elem = item.find("link")
                pub_date_elem = item.find("pubDate")
                desc_elem = item.find("description")
                title = title_elem.text if title_elem is not None else "No Title"
                link = link_elem.text if link_elem is not None else "#"
                pub_date_text = (
                    pub_date_elem.text if pub_date_elem is not None else None
                )
                formatted_date = "No date info"
                is_recent = True
                parsed_date = None
                if pub_date_text:
                    try:
                        parsed_date = email.utils.parsedate_to_datetime(pub_date_text)
                        formatted_date = parsed_date.strftime("%Y-%m-%d %H:%M")
                        is_recent = parsed_date >= two_days_ago
                    except Exception:
                        logger.warning("ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜")
                        formatted_date = pub_date_text
                if not is_recent:
                    continue
                source = None
                m = re.search(r"^(.*?)\s*-\s*([^-]+)$", title)
                if m:
                    source = m.group(2).strip()
                if not source and desc_elem is not None and desc_elem.text:
                    m = re.search(r'<font size="-1">([^<]+)</font>', desc_elem.text)
                    if m:
                        source = m.group(1).strip()
                    else:
                        m = re.search(r"<b>([^<]+)</b>", desc_elem.text)
                        if m:
                            source = m.group(1).strip()
                if not source:
                    source_elem = item.find("source")
                    if source_elem is not None:
                        source = source_elem.text.strip()
                news_items.append(
                    {
                        "title": title,
                        "link": link,
                        "published": formatted_date,
                        "published_raw": pub_date_text,
                        "published_datetime": parsed_date,
                        "source": source if source else "Unknown",
                        "query": query,
                        "summary": (
                            re.sub(r"<[^>]+>", "", desc_elem.text).strip()
                            if desc_elem is not None
                            else ""
                        ),
                    }
                )
                if len(news_items) >= max_items:
                    break
            self.news_cache[cache_key] = news_items
            return news_items
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []

    def get_article_content(self, url: str) -> tuple:
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                logger.error(f"HTTP ìƒíƒœ ì½”ë“œ {response.status_code}")
                return "ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", None

            soup = BeautifulSoup(response.content, "html.parser")
            source_meta = soup.find("meta", property="og:site_name")
            source = source_meta.get("content") if source_meta else None

            content = ""
            article = soup.find("article")
            if article:
                content = article.get_text(strip=True)
            if not content:
                meta_desc = soup.find("meta", {"name": "description"}) or soup.find(
                    "meta", property="og:description"
                )
                if meta_desc:
                    content = meta_desc.get("content", "")
            if not content:
                content = " ".join(
                    tag.get_text(strip=True)
                    for tag in soup.find_all(
                        ["p", "div"],
                        class_=re.compile(r"article|content|body|text|main", re.I),
                    )
                    if tag.get_text(strip=True)
                )
            if content:
                content = re.sub(r"\s+", " ", content).strip()
                content = re.sub(
                    r"(ê´‘ê³ |\[ê´‘ê³ \]|sponsored content|AD).*?(?=\s|$)",
                    "",
                    content,
                    flags=re.I,
                )
                return (
                    content[:2000] + "..." if len(content) > 2000 else content
                ), source
            return "ê¸°ì‚¬ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", source
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return "ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", None

    def summarize_with_openai(self, title: str, text: str) -> str:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        system_prompt = (
            "ë‹¹ì‹ ì€ ìµœê³ ì˜ ë‰´ìŠ¤ ì—ë””í„°ë¡œ, ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ëª…í™•í•˜ê³  í’ë¶€í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n\n"
            "ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ íŒŒì•…í•˜ì—¬ í†µí•©ëœ í•˜ë‚˜ì˜ ìš”ì•½ë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ì´ ìš”ì•½ë¬¸ì€ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:\n\n"
            "- ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œ ë° ì¤‘ìš”í•œ ì •ë³´\n"
            "- ì£¼ìš” ì‚¬ì‹¤, ê´€ë ¨ ë°ì´í„°, ì¤‘ìš”í•œ ì¸ìš©êµ¬\n"
            "- ë‰´ìŠ¤ì˜ ë§¥ë½ì  ì¤‘ìš”ì„±ê³¼ ì ì¬ì  ì˜í–¥\n\n"
            "ì‘ì„± ì‹œ ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¥´ì„¸ìš”:\n"
            "- ì •í™•ì„±, ê°„ê²°ì„±, ê°ê´€ì„±, ê°€ë…ì„±, ì™„ì „ì„±ì„ ê³ ë ¤\n"
            "ë¬¸ë‹¨ êµ¬ë¶„ì€ ìì—°ìŠ¤ëŸ½ê²Œ í•˜ë˜, ë²ˆí˜¸ë‚˜ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ì—†ì´ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ì „ì²´ ìš”ì•½ì€ ì •í™•íˆ 300ìë¡œ ì‘ì„±í•˜ì„¸ìš”. ì¤„ì„í‘œì‹œ(...)ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
        )
        data = {
            "model": os.getenv("GPT_MODEL", "gpt-4o-mini"),
            "messages": [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": f"ì œëª©: {title}\n\në‚´ìš©: {text}\n\nìœ„ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                },
            ],
            "temperature": 0.5,
            "max_tokens": 500,
            "frequency_penalty": 0.2,
            "presence_penalty": 0.1,
        }
        try:
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key}",
                },
                json=data,
                timeout=30,
            )
            if response.status_code == 200:
                result = response.json()
                summary = result["choices"][0]["message"]["content"].strip()
                self._update_api_usage_from_result(result, data["model"])
                return summary
            else:
                logger.error(f"OpenAI API ì˜¤ë¥˜: {response.status_code}")
                return f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ìƒíƒœ ì½”ë“œ: {response.status_code}"
        except Exception as e:
            logger.error(f"OpenAI API ìš”ì²­ ì˜¤ë¥˜: {e}")
            return "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ."

    def process_news_item(self, item: dict) -> dict:
        try:
            content, article_source = self.get_article_content(item["link"])
            if article_source and item["source"] == "Unknown":
                item["source"] = article_source
            item["ai_summary"] = self.summarize_with_openai(item["title"], content)
            return item
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return item

    def calculate_similarity_with_openai(self, news1: dict, news2: dict) -> float:
        key = tuple(sorted([news1["title"], news2["title"]]))
        if key in self.similarity_cache:
            return self.similarity_cache[key]
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 0.0
        prompt = f"""
ë‹¤ìŒ ë‘ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ì‹¬ì¸µ ë¶„ì„í•´ ì£¼ì„¸ìš”:

ê¸°ì‚¬ 1: {news1['title']}
ê¸°ì‚¬ 2: {news2['title']}

ì „ì²˜ë¦¬ëœ ì œëª©:
ê¸°ì‚¬ 1: {self.preprocess_title(news1['title'])}
ê¸°ì‚¬ 2: {self.preprocess_title(news2['title'])}

ë‹¨ìˆœíˆ ë™ì¼í•œ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ê°€ ì•„ë‹ˆë¼, ë‘ ì œëª©ì´ ê°™ì€ ì£¼ì œë‚˜ ì‚¬ê±´ì„ ë‹¤ë£¨ê³  ìˆëŠ”ì§€ì˜ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±ì— ì¤‘ì ì„ ë‘ê³  í‰ê°€í•´ ì£¼ì„¸ìš”.

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”:
1. ë‘ ì œëª©ì´ ë™ì¼í•œ ì£¼ìš” ì‚¬ê±´/ì´ìŠˆë¥¼ ë‹¤ë£¨ê³  ìˆëŠ”ê°€? (ê°€ì¥ ì¤‘ìš”)
2. ë‘ ì œëª©ì—ì„œ ì–¸ê¸‰ëœ ì£¼ì²´(ì¸ë¬¼, ì¡°ì§, ê¸°ê´€ ë“±)ê°€ ë™ì¼í•œê°€?
3. ë‘ ì œëª©ì˜ í•µì‹¬ ë©”ì‹œì§€ë‚˜ ê´€ì ì´ ìœ ì‚¬í•œê°€?
4. í‘œí˜„ ë°©ì‹ë§Œ ë‹¤ë¥¼ ë¿ ì „ë‹¬í•˜ëŠ” ì •ë³´ê°€ ë³¸ì§ˆì ìœ¼ë¡œ ë™ì¼í•œê°€?
5. ë™ì¼í•œ ë‰´ìŠ¤ë¥¼ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë³´ë„í•œ ê²ƒì¸ê°€?

0.0(ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œ) ~ 1.0(ê±°ì˜ ë™ì¼í•œ ë‚´ìš©) ì‚¬ì´ì˜ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”:
- 0.0~0.3: ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œë‚˜ ì‚¬ê±´ì„ ë‹¤ë£¸
- 0.4~0.6: ê´€ë ¨ëœ ì£¼ì œì§€ë§Œ ë‹¤ë¥¸ ì¸¡ë©´ì´ë‚˜ ê´€ì ì„ ë‹¤ë£¸
- 0.7~0.9: ê°™ì€ ì£¼ì œ/ì‚¬ê±´ì„ ë‹¤ë£¨ë©° ìœ ì‚¬í•œ ì •ë³´ ì „ë‹¬
- 0.9~1.0: ì‚¬ì‹¤ìƒ ë™ì¼í•œ ë‰´ìŠ¤(ë‹¤ë¥¸ í‘œí˜„ë§Œ ì‚¬ìš©)

í‰ê°€ í›„, ì˜¤ì§ 0ë¶€í„° 1 ì‚¬ì´ì˜ ì†Œìˆ˜ì  ë‘˜ì§¸ ìë¦¬ í˜•ì‹(ì˜ˆ: 0.85)ì˜ ìˆ«ìë§Œ ì‘ë‹µí•´ ì£¼ì„¸ìš”.
"""
        data = {
            "model": os.getenv("GPT_MODEL", "gpt-4o-mini"),
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ ìœ ì‚¬ì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì˜¤ì§ ìˆ«ìë§Œ ì‘ë‹µí•˜ì„¸ìš”.",
                },
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.2,
            "max_tokens": 10,
        }
        try:
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key}",
                },
                json=data,
                timeout=20,
            )
            if response.status_code == 200:
                result = response.json()
                similarity_text = result["choices"][0]["message"]["content"].strip()
                match = re.search(r"(\d+\.\d{2}|\d+)", similarity_text)
                similarity = float(match.group(1)) if match else 0.0
                similarity = max(0.0, min(1.0, similarity))
                self._update_api_usage_from_result(result, data["model"])
                self.similarity_cache[key] = similarity
                logger.info(
                    f"ë¹„êµ: '{news1['title']}' vs '{news2['title']}' -> AI ìœ ì‚¬ë„: {similarity:.2f}"
                )
                return similarity
            else:
                logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° API ì˜¤ë¥˜: {response.status_code}")
                return 0.0
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return 0.0

    def filter_duplicate_news(
        self, news_items: list, similarity_threshold: float = 0.7
    ) -> list:
        if not news_items or len(news_items) <= 1:
            return news_items

        unique_news = [news_items[0]]
        total_comparisons = len(news_items) - 1
        SIMPLE_SIM_HIGH = 0.6
        SIMPLE_SIM_LOW = 0.25

        with tqdm(
            total=total_comparisons,
            **get_tqdm_settings("ì¤‘ë³µ ë‰´ìŠ¤ í•„í„°ë§", COLORS["MAGENTA"], 3),
        ) as pbar:
            for i in range(1, len(news_items)):
                is_duplicate = False
                current_title = news_items[i]["title"]
                pbar.set_description(f"ë¹„êµ ì¤‘: {current_title[:30]}...")
                for unique_item in unique_news:
                    basic_sim = self.simple_jaccard_similarity(
                        current_title, unique_item["title"]
                    )
                    logger.info(
                        f"ë¹„êµ: '{current_title}' vs '{unique_item['title']}' -> ê¸°ë³¸ ìœ ì‚¬ë„: {basic_sim:.2f}"
                    )
                    if basic_sim >= SIMPLE_SIM_HIGH:
                        is_duplicate = True
                        logger.info(
                            f"ìë™ ì¤‘ë³µ ê°ì§€ (ê¸°ë³¸ ìœ ì‚¬ë„ {basic_sim:.2f}): {current_title}"
                        )
                        break
                    elif basic_sim <= SIMPLE_SIM_LOW:
                        continue
                    else:
                        ai_sim = self.calculate_similarity_with_openai(
                            news_items[i], unique_item
                        )
                        pbar.set_postfix_str(
                            f"ìì¹´ë“œ: {basic_sim:.2f} / AI: {ai_sim:.2f} / ê¸°ì¤€: {similarity_threshold}"
                        )
                        if ai_sim >= similarity_threshold:
                            is_duplicate = True
                            logger.info(
                                f"AI ì¤‘ë³µ ê°ì§€: {current_title} (AI ìœ ì‚¬ë„: {ai_sim:.2f})"
                            )
                            break
                if not is_duplicate:
                    unique_news.append(news_items[i])
                pbar.update(1)
        logger.info(f"ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°: {len(news_items) - len(unique_news)}ê°œ ì œê±°ë¨")
        return unique_news


# =============================================================================
# EmailService í´ë˜ìŠ¤: Gmail API ë° SMTP+OAuth2 ë°©ì‹ ì´ë©”ì¼ ë°œì†¡
# =============================================================================
class EmailService:
    def __init__(self):
        self.sender_email: str = os.getenv("EMAIL_USERNAME")

    def get_gmail_service(self):
        SCOPES = ["https://www.googleapis.com/auth/gmail.send"]
        creds = None
        token_json_path = os.getenv("EMAIL_OAUTH2_TOKEN", "token.json")

        if os.path.exists(token_json_path):
            try:
                logger.info("token.jsonì—ì„œ ì¸ì¦ ì •ë³´ ë¡œë“œ ì¤‘")
                with open(token_json_path, "r") as token_file:
                    token_data = json.load(token_file)
                    creds = Credentials(
                        token=token_data.get("token"),
                        refresh_token=token_data.get("refresh_token"),
                        token_uri=token_data.get(
                            "token_uri", "https://oauth2.googleapis.com/token"
                        ),
                        client_id=token_data.get("client_id"),
                        client_secret=token_data.get("client_secret"),
                        scopes=token_data.get("scopes", SCOPES),
                    )
            except Exception as e:
                logger.warning(f"token.json ë¡œë“œ ì˜¤ë¥˜: {e}")
                creds = None

        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                logger.info("í† í° ê°±ì‹  ì¤‘...")
                creds.refresh(Request())
            else:
                logger.info("ìƒˆ OAuth ì¸ì¦ íë¦„ ì‹œì‘ ì¤‘...")
                credentials_file = os.getenv(
                    "GMAIL_CREDENTIALS_FILE", "credentials.json"
                )
                if not os.path.exists(credentials_file):
                    logger.error(f"OAuth ì¸ì¦ íŒŒì¼ ì—†ìŒ: {credentials_file}")
                    return None
                try:
                    with open(credentials_file, "r") as f:
                        client_config = json.load(f)
                    if "installed" in client_config:
                        client_config["installed"]["redirect_uris"] = [
                            "urn:ietf:wg:oauth:2.0:oob"
                        ]
                    elif "web" in client_config:
                        client_config["web"]["redirect_uris"] = [
                            "urn:ietf:wg:oauth:2.0:oob"
                        ]
                    flow = InstalledAppFlow.from_client_config(
                        client_config, SCOPES, redirect_uri="urn:ietf:wg:oauth:2.0:oob"
                    )
                    auth_url, _ = flow.authorization_url(
                        access_type="offline", include_granted_scopes="true"
                    )
                    print("=" * 70)
                    print(
                        "Google ê³„ì • ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. ì•„ë˜ URLì„ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê³  ì¸ì¦ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:"
                    )
                    print("=" * 70)
                    print(auth_url)
                    print("=" * 70)
                    auth_code = input("ì¸ì¦ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                    flow.fetch_token(code=auth_code)
                    creds = flow.credentials
                    print("ì¸ì¦ ì„±ê³µ! ìƒˆ í† í°ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.error(f"OAuth ì¸ì¦ ì˜¤ë¥˜: {e}")
                    return None
            try:
                logger.info("ìƒˆ ì¸ì¦ ì •ë³´ë¥¼ token.jsonì— ì €ì¥ ì¤‘")
                token_data = {
                    "token": creds.token,
                    "refresh_token": creds.refresh_token,
                    "token_uri": creds.token_uri,
                    "client_id": creds.client_id,
                    "client_secret": creds.client_secret,
                    "scopes": list(creds.scopes),
                }
                with open(token_json_path, "w") as token_file:
                    json.dump(token_data, token_file)
            except Exception as e:
                logger.error(f"í† í° ì €ì¥ ì˜¤ë¥˜: {e}")
        try:
            service = build("gmail", "v1", credentials=creds)
            return service
        except Exception as e:
            logger.error(f"Gmail ì„œë¹„ìŠ¤ ìƒì„± ì˜¤ë¥˜: {e}")
            return None

    def send_email(
        self, recipient_name: str, recipient_email: str, subject: str, html_content: str
    ) -> bool:
        if not self.sender_email:
            logger.error("ë°œì‹ ì ì´ë©”ì¼ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False
        try:
            service = self.get_gmail_service()
            if not service:
                logger.error("Gmail API ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            message = MIMEMultipart("alternative")
            message["Subject"] = subject
            message["From"] = self.sender_email
            message["To"] = recipient_email
            message.attach(MIMEText(html_content, "html"))
            raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode()
            sent_message = (
                service.users()
                .messages()
                .send(userId="me", body={"raw": raw_message})
                .execute()
            )
            logger.info(
                f"ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ: {recipient_email} (ID: {sent_message['id']})"
            )
            return True
        except Exception as e:
            logger.error(f"Gmail API ì´ë©”ì¼ ë°œì†¡ ì˜¤ë¥˜: {e}")
            try:
                logger.warning("SMTP+OAuth2 ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
                oauth2_token = os.getenv("EMAIL_OAUTH2_TOKEN")
                if not oauth2_token:
                    logger.error("OAuth2 í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return False
                smtp_server = os.getenv("EMAIL_SMTP_SERVER", "smtp.gmail.com")
                smtp_port = int(os.getenv("EMAIL_SMTP_PORT", "587"))
                import smtplib

                server = smtplib.SMTP(smtp_server, smtp_port)
                server.starttls()
                auth_string = (
                    f"user={self.sender_email}\1auth=Bearer {oauth2_token}\1\1"
                )
                server.docmd(
                    "AUTH", "XOAUTH2 " + base64.b64encode(auth_string.encode()).decode()
                )
                server.send_message(message)
                server.quit()
                logger.info(f"SMTP ë°©ì‹ ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ: {recipient_email}")
                return True
            except Exception as smtp_error:
                logger.error(f"SMTP ì´ë©”ì¼ ë°œì†¡ ì˜¤ë¥˜: {smtp_error}")
                return False


# =============================================================================
# SubscriberManager í´ë˜ìŠ¤: êµ¬ë…ì ë¡œë“œ, ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì´ë©”ì¼ ë°œì†¡ ë‹´ë‹¹
# =============================================================================
class SubscriberManager:
    def __init__(self, news_fetcher: NewsFetcher, email_service: EmailService):
        self.news_fetcher = news_fetcher
        self.email_service = email_service

    @staticmethod
    def load_subscribers(file_path: str = "subscribers.txt") -> list:
        subscribers = []
        if not os.path.exists(file_path):
            logger.error(f"êµ¬ë…ì íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return subscribers
        try:
            logger.info(f"êµ¬ë…ì íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
            with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                current_sub = {}
                for line in f:
                    line = line.strip()
                    if not line:
                        if current_sub.get("email") and current_sub.get("topics"):
                            subscribers.append(current_sub)
                        current_sub = {}
                        continue
                    if line.startswith("ID:"):
                        if current_sub.get("email") and current_sub.get("topics"):
                            subscribers.append(current_sub)
                        current_sub = {
                            "id": line[3:].strip(),
                            "name": "",
                            "email": "",
                            "topics": [],
                        }
                    elif line.startswith("ì´ë¦„:") and current_sub:
                        current_sub["name"] = line[3:].strip()
                    elif line.startswith("ì´ë©”ì¼:") and current_sub:
                        current_sub["email"] = line[4:].strip()
                    elif line.startswith("í† í”½:") and current_sub:
                        topics = [topic.strip() for topic in line[3:].split(",")]
                        current_sub["topics"] = topics
                if current_sub.get("email") and current_sub.get("topics"):
                    subscribers.append(current_sub)
            logger.info(f"ì´ {len(subscribers)}ëª…ì˜ êµ¬ë…ì ë¡œë“œ ì™„ë£Œ")
            return subscribers
        except Exception as e:
            logger.error(f"êµ¬ë…ì ì •ë³´ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return []

    @staticmethod
    def log_news_titles(news_items: list, prefix: str = "") -> None:
        if not news_items:
            logger.warning(f"{prefix} ë‰´ìŠ¤ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        logger.info(f"{prefix} ë‰´ìŠ¤ ì œëª© ëª©ë¡ (ì´ {len(news_items)}ê°œ):")
        for idx, item in enumerate(news_items, 1):
            topic = item.get("query", "ê¸°íƒ€")
            logger.info(f"{idx}. [{topic}] {item['title']}")

    def fetch_news_for_subscriber(self, subscriber: dict) -> list:
        collected_news = []
        topics = subscriber.get("topics", [])
        unique_topics = list(dict.fromkeys(topics))
        name = subscriber.get("name", "êµ¬ë…ì")

        if len(unique_topics) < len(topics):
            logger.info(
                f"{name}ì˜ í† í”½ ëª©ë¡ì—ì„œ {len(topics) - len(unique_topics)}ê°œ ì¤‘ë³µ ì œê±°ë¨"
            )

        with tqdm(
            total=len(unique_topics),
            **get_tqdm_settings(f"{name}ì˜ í† í”½ ì²˜ë¦¬", COLORS["BLUE"], 1, True),
        ) as topic_bar:
            for topic in unique_topics:
                topic_bar.set_description(
                    f"{COLORS['CYAN']}í† í”½ '{topic}' ì²˜ë¦¬ ì¤‘{COLORS['RESET']}"
                )
                logger.info(f"í† í”½ '{topic}' ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
                try:
                    news_items = self.news_fetcher.get_google_news(query=topic)
                    if news_items:
                        logger.info(f"í† í”½ '{topic}'ì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                        unique_news = self.news_fetcher.filter_duplicate_news(
                            news_items, similarity_threshold=0.65
                        )
                        logger.info(
                            f"í† í”½ '{topic}'ì—ì„œ {len(unique_news)}ê°œ ê³ ìœ  ë‰´ìŠ¤ ì„ ì •"
                        )
                        with ThreadPoolExecutor(max_workers=3) as executor:
                            futures = {
                                executor.submit(
                                    self.news_fetcher.process_news_item, item
                                ): item
                                for item in unique_news
                            }
                            with tqdm(
                                total=len(futures),
                                **get_tqdm_settings(
                                    f"'{topic}' ë‰´ìŠ¤ ì²˜ë¦¬", COLORS["GREEN"], 2
                                ),
                            ) as news_bar:
                                for future in as_completed(futures):
                                    try:
                                        processed_item = future.result()
                                        collected_news.append(processed_item)
                                        news_bar.update(1)
                                        news_bar.set_description(
                                            f"{COLORS['YELLOW']}ì²˜ë¦¬: {processed_item['title'][:20]}...{COLORS['RESET']}"
                                        )
                                    except Exception as e:
                                        logger.error(f"ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                                        news_bar.update(1)
                    else:
                        logger.warning(f"í† í”½ '{topic}' ë‰´ìŠ¤ ì—†ìŒ")
                except Exception as e:
                    logger.error(f"í† í”½ '{topic}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                topic_bar.update(1)
        return collected_news

    def remove_duplicates_from_news(
        self, news_items: list, similarity_threshold: float = 0.65
    ) -> list:
        if not news_items or len(news_items) <= 1:
            return news_items

        topic_groups = defaultdict(list)
        for item in news_items:
            topic_groups[item.get("query", "ê¸°íƒ€")].append(item)

        cleaned_by_topic = {}
        for topic, items in topic_groups.items():
            logger.info(f"í† í”½ '{topic}' ë‚´ë¶€ ì¤‘ë³µ ì œê±° ì‹œì‘ (ì´ {len(items)}ê°œ)")
            unique_items = []
            processed_ids = set()
            for item in items:
                item_id = item.get("link", item.get("title", ""))
                if item_id not in processed_ids:
                    unique_items.append(item)
                    processed_ids.add(item_id)
            cleaned_items = self.news_fetcher.filter_duplicate_news(
                unique_items, similarity_threshold=0.7
            )
            cleaned_by_topic[topic] = cleaned_items
            logger.info(f"í† í”½ '{topic}': {len(cleaned_items)}ê°œ ë‚¨ìŒ")

        all_news = []
        processed_ids = set()
        for items in cleaned_by_topic.values():
            for item in items:
                item_id = item.get("link", item.get("title", ""))
                if item_id not in processed_ids:
                    all_news.append(item)
                    processed_ids.add(item_id)

        final_news = self.news_fetcher.filter_duplicate_news(
            all_news, similarity_threshold
        )
        for topic, items in cleaned_by_topic.items():
            count_in_final = sum(
                1 for item in final_news if item.get("query", "ê¸°íƒ€") == topic
            )
            if count_in_final < 5:
                needed = 5 - count_in_final
                logger.info(f"í† í”½ '{topic}'ì— ì¶”ê°€ {needed}ê°œ ë‰´ìŠ¤ í•„ìš”")
                candidates = [item for item in items if item not in final_news]
                final_news.extend(candidates[:needed])

        topic_final = defaultdict(list)
        for item in final_news:
            topic_final[item.get("query", "ê¸°íƒ€")].append(item)
        final_news_clipped = []
        for items in topic_final.values():
            final_news_clipped.extend(items[:7] if len(items) > 7 else items)
        logger.info(f"ìµœì¢… ë‰´ìŠ¤ í•­ëª©: {len(final_news_clipped)}ê°œ")
        return final_news_clipped

    def process_subscriber(self, subscriber: dict, collected_news: list) -> dict:
        name = subscriber.get("name", "êµ¬ë…ì")
        email_addr = subscriber.get("email", "")
        topics = subscriber.get("topics", [])
        if not topics:
            logger.warning(f"{name}ì˜ ê´€ì‹¬ í† í”½ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "name": name,
                "email": email_addr,
                "success": False,
                "reason": "ê´€ì‹¬ í† í”½ ì—†ìŒ",
            }
        if not collected_news:
            logger.warning(f"{name}ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "name": name,
                "email": email_addr,
                "success": False,
                "reason": "ë‰´ìŠ¤ ì—†ìŒ",
            }

        logger.info(f"{name} ì´ë©”ì¼ ì½˜í…ì¸  ì¤€ë¹„ (ìˆ˜ì§‘ ë‰´ìŠ¤: {len(collected_news)}ê°œ)")
        deduped_news = self.remove_duplicates_from_news(
            collected_news, similarity_threshold=0.65
        )
        self.log_news_titles(deduped_news, prefix=f"{name} ìµœì¢… ë‰´ìŠ¤ ëª©ë¡")

        topic_news = defaultdict(list)
        for item in deduped_news:
            for topic in topics:
                if topic == item.get("query", "ê¸°íƒ€"):
                    topic_news[topic].append(item)

        if not topic_news:
            logger.warning(f"{name}ì˜ ê´€ì‹¬ í† í”½ê³¼ ì¼ì¹˜í•˜ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "name": name,
                "email": email_addr,
                "success": False,
                "reason": "ê´€ì‹¬ ë‰´ìŠ¤ ì—†ìŒ",
            }

        for topic in topic_news:
            topic_news[topic] = topic_news[topic][:7]

        current_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        html_content = self.build_email_html(name, current_date, topic_news)
        subject = f"{name}ë‹˜ì„ ìœ„í•œ ê´€ì‹¬ í† í”½ ë‰´ìŠ¤ ìš”ì•½ ({current_date})"
        success = self.email_service.send_email(name, email_addr, subject, html_content)
        return {
            "name": name,
            "email": email_addr,
            "success": success,
            "news_count": len(collected_news),
        }

    @staticmethod
    def build_email_html(name: str, current_date: str, topic_news: dict) -> str:
        html = f"""<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>{name}ë‹˜ì„ ìœ„í•œ ë§ì¶¤ ë‰´ìŠ¤ ìš”ì•½</title>
  <style>
    * {{ margin: 0; padding: 0; box-sizing: border-box; }}
    body {{ font-family: 'Noto Sans KR', sans-serif; color: #1f2937; line-height: 1.5; background-color: #f8fafc; }}
  </style>
</head>
<body>
  <div style="background: linear-gradient(to right, #2563eb, #3b82f6); padding: 24px; text-align: center; color: white;">
    <h1 style="font-size: 24px; font-weight: 700;">{name}ë‹˜ì„ ìœ„í•œ ë§ì¶¤ ë‰´ìŠ¤</h1>
    <p style="font-size: 16px;">{current_date} ë°œí–‰</p>
  </div>
  <div style="max-width: 700px; margin: 0 auto; padding: 24px; background: white; border-radius: 8px;">
    <div style="margin-bottom: 24px; padding: 16px; background: #f0f9ff; border-left: 4px solid #3b82f6;">
      <p style="color: #1e40af; font-size: 16px;">ì•ˆë…•í•˜ì„¸ìš”, <strong>{name}</strong>ë‹˜!</p>
      <p style="color: #1e40af; font-size: 14px; margin-top: 8px;">ìµœê·¼ ê´€ì‹¬ í† í”½ ë‰´ìŠ¤ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.</p>
    </div>
"""
        count = 0
        for topic, news_list in topic_news.items():
            count += 1
            background = "#f8fafc" if count % 2 == 0 else "#ffffff"
            html += f"""
    <div style="margin-bottom: 32px; padding-bottom: 24px; border-bottom: 1px solid #e5e7eb;">
      <div style="display: flex; align-items: center; margin-bottom: 16px;">
        <div style="background: #3b82f6; border-radius: 9999px; width: 24px; height: 24px; display: flex; align-items: center; justify-content: center; margin-right: 8px;">
          <span style="color: white; font-weight: bold;">{count}</span>
        </div>
        <h2 style="color: #1e40af; font-size: 20px; font-weight: 600;">{topic}</h2>
      </div>
"""
            if not news_list:
                html += f"""
      <div style="margin-bottom: 24px; padding: 16px; background: {background}; border: 1px solid #e5e7eb; border-radius: 8px; text-align: center;">
        <p style="color: #6b7280; font-size: 14px;">ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.</p>
      </div>
"""
            else:
                for item in news_list:
                    summary = item.get(
                        "ai_summary", item.get("summary", "ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    )
                    summary = summary if len(summary) <= 300 else summary[:300]
                    html += f"""
      <div style="margin-bottom: 24px; padding: 16px; background: {background}; border: 1px solid #e5e7eb; border-radius: 8px;">
        <h3 style="font-size: 18px; font-weight: 600; color: #1e3a8a; margin-bottom: 8px;">
          <a href="{item['link']}" target="_blank" style="text-decoration: none; color: inherit;">{item['title']}</a>
        </h3>
        <div style="font-size: 13px; color: #6b7280; margin-bottom: 12px;">
          <span>{item['source']}</span> | <span>{item['published']}</span>
        </div>
        <div style="font-size: 14px; line-height: 1.6; color: #4b5563; margin-bottom: 16px;">{summary}</div>
      </div>
"""
            html += "    </div>\n"
        html += f"""
    <div style="text-align: center; font-size: 12px; color: #6b7280; margin-top: 32px; border-top: 1px solid #e5e7eb; padding-top: 16px;">
      <p>ì´ ì´ë©”ì¼ì€ ìë™ìœ¼ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
      <p>Â© {datetime.now().year} ë‰´ìŠ¤ë ˆí„° ì„œë¹„ìŠ¤ | {current_date}</p>
    </div>
  </div>
  <div style="background: #1e40af; padding: 16px; text-align: center; color: white; font-size: 12px; margin-top: 24px;">
    <p>êµ¬ë… ê´€ë ¨ ë¬¸ì˜ëŠ” ê´€ë¦¬ìì—ê²Œ ì—°ë½í•˜ì„¸ìš”.</p>
    <p>ì´ ë©”ì¼ì€ ë°œì‹  ì „ìš©ì…ë‹ˆë‹¤.</p>
  </div>
</body>
</html>
"""
        return html


# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================
def main() -> None:
    try:
        logger.info("ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì´ë©”ì¼ ë°œì†¡ ì‹œì‘")
        news_fetcher = NewsFetcher()
        email_service = EmailService()
        subscriber_manager = SubscriberManager(news_fetcher, email_service)
        subscribers = subscriber_manager.load_subscribers()
        if not subscribers:
            logger.error("êµ¬ë…ìê°€ ì—†ìŠµë‹ˆë‹¤. subscribers.txt íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            return

        all_topics = set()
        for subscriber in subscribers:
            topics = subscriber.get("topics", [])
            all_topics.update(topics)
        logger.info(
            f"ì „ì²´ {len(subscribers)}ëª… êµ¬ë…ì, ê³ ìœ  í† í”½ {len(all_topics)}ê°œ ë°œê²¬"
        )

        topic_news_cache = {}

        with tqdm(
            total=len(subscribers),
            **get_tqdm_settings("ì „ì²´ êµ¬ë…ì ì²˜ë¦¬", COLORS["CYAN"], 0, True),
        ) as main_bar:
            for subscriber in subscribers:
                name = subscriber.get("name", "Unknown")
                email_addr = subscriber.get("email", "")
                topics = subscriber.get("topics", [])
                main_bar.set_description(
                    f"{COLORS['BLUE']}êµ¬ë…ì '{name}' ì²˜ë¦¬ ì¤‘{COLORS['RESET']}"
                )
                if not topics or not email_addr:
                    logger.warning(
                        f"{name}ì˜ ê´€ì‹¬ í† í”½ ë˜ëŠ” ì´ë©”ì¼ ì£¼ì†Œê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤."
                    )
                    main_bar.update(1)
                    continue

                logger.info(f"{name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
                cached_subscriber_news = []
                topics_to_fetch = []
                for topic in topics:
                    if topic in topic_news_cache:
                        logger.info(
                            f"ìºì‹œì—ì„œ í† í”½ '{topic}'ì˜ ë‰´ìŠ¤ {len(topic_news_cache[topic])}ê°œ ë¡œë“œ"
                        )
                        cached_subscriber_news.extend(topic_news_cache[topic])
                    else:
                        topics_to_fetch.append(topic)

                if topics_to_fetch:
                    temp_subscriber = {
                        "name": name,
                        "email": email_addr,
                        "topics": topics_to_fetch,
                    }
                    new_news = subscriber_manager.fetch_news_for_subscriber(
                        temp_subscriber
                    )
                    for item in new_news:
                        topic = item.get("query", "ê¸°íƒ€")
                        if topic in topics_to_fetch:
                            topic_news_cache.setdefault(topic, []).append(item)
                    cached_subscriber_news.extend(new_news)

                subscriber_news = cached_subscriber_news

                if subscriber_news:
                    logger.info(f"{name}ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(subscriber_news)}ê°œ")
                    subscriber_manager.log_news_titles(
                        subscriber_news, prefix=f"{name} ì´ˆê¸° ìˆ˜ì§‘"
                    )
                    result = subscriber_manager.process_subscriber(
                        subscriber, subscriber_news
                    )
                    if result.get("success"):
                        logger.info(f"{name}ì—ê²Œ ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ")
                        main_bar.set_postfix_str(
                            f"ë‰´ìŠ¤: {len(subscriber_news)} / ë°œì†¡: ì„±ê³µ"
                        )
                    else:
                        logger.error(
                            f"{name} ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {result.get('reason', 'ì•Œ ìˆ˜ ì—†ìŒ')}"
                        )
                        main_bar.set_postfix_str(
                            f"ë‰´ìŠ¤: {len(subscriber_news)} / ë°œì†¡: ì‹¤íŒ¨"
                        )
                else:
                    logger.warning(f"{name}ì˜ ê´€ì‹¬ í† í”½ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    main_bar.set_postfix_str("ë‰´ìŠ¤: 0")
                time.sleep(1)
                main_bar.update(1)

        stats = news_fetcher.api_usage_stats
        logger.info("=" * 50)
        logger.info("API ì‚¬ìš©ëŸ‰ í†µê³„")
        logger.info("=" * 50)
        logger.info(f"ì´ API í˜¸ì¶œ íšŸìˆ˜: {stats['api_calls']}íšŒ")
        logger.info(f"ì´ ì‚¬ìš© í† í°: {stats['total_tokens']:,}ê°œ")
        logger.info(f"  - í”„ë¡¬í”„íŠ¸ í† í°: {stats['prompt_tokens']:,}ê°œ")
        logger.info(f"  - ì‘ë‹µ í† í°: {stats['completion_tokens']:,}ê°œ")
        logger.info(
            f"ì´ ë¹„ìš©: ${stats['total_cost_usd']:.4f} (ì•½ â‚©{stats['total_cost_krw']:.0f})"
        )

        logger.info("-" * 50)
        logger.info("ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰:")
        for model, model_stats in stats["models"].items():
            logger.info(f"  â–¶ {model}:")
            logger.info(f"    - API í˜¸ì¶œ: {model_stats['api_calls']}íšŒ")
            logger.info(f"    - ì´ í† í°: {model_stats['total_tokens']:,}ê°œ")
            logger.info(
                f"    - ë¹„ìš©: ${model_stats['total_cost_usd']:.4f} (ì•½ â‚©{model_stats['total_cost_krw']:.0f})"
            )

        logger.info("=" * 50)
        logger.info("ëª¨ë“  êµ¬ë…ì ì²˜ë¦¬ ì™„ë£Œ")

        print("\n")
        print(f"{COLORS['CYAN']}{'=' * 60}{COLORS['RESET']}")
        print(f"{COLORS['CYAN']}ğŸ’° API ì‚¬ìš©ëŸ‰ í†µê³„ ìš”ì•½{COLORS['RESET']}")
        print(f"{COLORS['CYAN']}{'=' * 60}{COLORS['RESET']}")
        print(
            f"{COLORS['WHITE']}ğŸ“Š ì´ API í˜¸ì¶œ: {COLORS['YELLOW']}{stats['api_calls']:,}íšŒ{COLORS['RESET']}"
        )
        print(
            f"{COLORS['WHITE']}ğŸ”¤ ì´ ì‚¬ìš© í† í°: {COLORS['YELLOW']}{stats['total_tokens']:,}ê°œ{COLORS['RESET']}"
        )
        print(
            f"{COLORS['WHITE']}  - ì…ë ¥ í† í°: {COLORS['GREEN']}{stats['prompt_tokens']:,}ê°œ{COLORS['RESET']}"
        )
        print(
            f"{COLORS['WHITE']}  - ì¶œë ¥ í† í°: {COLORS['GREEN']}{stats['completion_tokens']:,}ê°œ{COLORS['RESET']}"
        )
        print(
            f"{COLORS['WHITE']}ğŸ’µ ì´ ë¹„ìš©: {COLORS['MAGENTA']}${stats['total_cost_usd']:.4f} (ì•½ â‚©{stats['total_cost_krw']:.0f}){COLORS['RESET']}"
        )
        print(f"{COLORS['CYAN']}{'-' * 60}{COLORS['RESET']}")
        print(f"{COLORS['CYAN']}ğŸ“‹ ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰:{COLORS['RESET']}")
        for model, model_stats in stats["models"].items():
            print(f"{COLORS['BLUE']}  â–¶ {model}:{COLORS['RESET']}")
            print(
                f"{COLORS['WHITE']}    - API í˜¸ì¶œ: {COLORS['YELLOW']}{model_stats['api_calls']}íšŒ{COLORS['RESET']}"
            )
            print(
                f"{COLORS['WHITE']}    - ì´ í† í°: {COLORS['YELLOW']}{model_stats['total_tokens']:,}ê°œ{COLORS['RESET']}"
            )
            print(
                f"{COLORS['WHITE']}    - ë¹„ìš©: {COLORS['MAGENTA']}${model_stats['total_cost_usd']:.4f} (ì•½ â‚©{model_stats['total_cost_krw']:.0f}){COLORS['RESET']}"
            )
        print(f"{COLORS['CYAN']}{'=' * 60}{COLORS['RESET']}")
        print(f"\n{COLORS['GREEN']}âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!{COLORS['RESET']}")

    except Exception as e:
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        colorama.deinit()


if __name__ == "__main__":
    main()
